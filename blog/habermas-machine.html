
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Habermas Machine | Mosaic Labs</title>
  <meta name="description" content="Explanation of how DeepMind's AI mediator helps groups converge on shared beliefs.">
  <link rel="icon" type="image/png" href="../favicon.png">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Lora:ital,wght@0,400..700;1,400..700&display=swap" rel="stylesheet">
  <link rel="stylesheet" type="text/css" href="../css/style.css">

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/auto-render.min.js"></script>

  <meta property="og:type" content="article">
  <meta property="og:url" content="https://www.mosaic-labs.org/blog/habermas-machine">
  <meta property="og:title" content="Habermas Machine | Mosaic Labs">
  <meta property="og:description" content="Explanation of how DeepMind's AI mediator helps groups converge on shared beliefs.">
  <meta property="og:image" content="https://mosaic-labs.org/images/og-image.jpg">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">

  <meta property="twitter:card" content="summary_large_image">
  <meta property="twitter:url" content="https://www.mosaic-labs.org/blog/habermas-machine">
  <meta property="twitter:title" content="Habermas Machine | Mosaic Labs">
  <meta property="twitter:description" content="Explanation of how DeepMind's AI mediator helps groups converge on shared beliefs.">
  <meta property="twitter:image" content="https://mosaic-labs.org/images/og-image.jpg">

  <script src="../components/gtag-component.js" defer></script>
  <script src="../components/footer-component.js" defer></script>
  <script src="../components/newsletter-component.js" defer></script>
</head>
<body>
  <div class="content">
    <header>
      <a href="/blog/">← Back to Blog</a>
    </header>
    <article>
      <h1>Habermas Machine</h1>
      <div class="post-author">Nicholas Kees Dupuis</div>
      <div class="post-metadata">
        <time datetime="2025-03-12">March 12, 2025</time>
      </div>
      <p>This post is a distillation of a recent work in AI-assisted human coordination from Google DeepMind.  </p>
<p>The <a href="https://www.science.org/doi/10.1126/science.adq2852">paper</a> has received some <a href="https://www.theguardian.com/technology/2024/oct/17/ai-mediation-tool-may-help-reduce-culture-war-rifts-say-researchers">press</a> <a href="https://www.technologyreview.com/2024/10/17/1105810/ai-could-help-people-find-common-ground-during-deliberations/">attention</a>, and anecdotally, it has become the de-facto example that people bring up of <strong>AI used to improve group discussions</strong>.  </p>
<p>Since this work represents a particular perspective/bet on how advanced AI could help improve human coordination, the following explainer is to bring anyone curious up to date. I’ll be referencing both the published paper as well as the <a href="https://www.science.org/doi/suppl/10.1126/science.adq2852/suppl_file/science.adq2852_sm.pdf">supplementary materials</a>.</p>
<h1>Summary</h1>
<p>The Habermas Machine<sup><a id="footnote-ref-vgzvc1nrpo" href="#footnote-vgzvc1nrpo" data-footnote-ref aria-describedby="footnote-label">1</a></sup> (HM) is a scaffolded pair of LLMs designed to <strong>find consensus among people who disagree</strong>, and help them converge to a common point of view. Human participants are asked to give their opinions in response to a binary question (E.g. “Should voting be compulsory?”). Participants give their level of agreement<sup><a id="footnote-ref-txqquee37d" href="#footnote-txqquee37d" data-footnote-ref aria-describedby="footnote-label">2</a></sup>, as well as write a short 3-10 sentence opinion statement representing their view on the issue. </p>
<p>The system takes the individual statements of opinion from different people and outputs a single “group statement,” optimized for broad endorsement. During a session, the system refines its output iteratively, incorporating live human feedback to help it converge on a widely acceptable perspective.</p>
<p>At the end of the session, <strong>participants are asked if their position on the question changed</strong>, to assess if the group moved toward consensus. </p>
<p><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXc76NU2Es27AH8BJGWGNomzOoY6IH4hswafre0mu9q6WSIB7WYcwV6c9e8W7DTt7feJiaDrcRncr5-ycJmopJg8Bj2OyjUcANK5q1gG4V5KzVBhm3iPX7ukoVTAB9MkszXdvKZUrA?key=aWp0E8rlx3pi0wuSPlVVoKs7" alt=""></p>
<h2>Full Process</h2>
<p>The method starts with a question and ends with a statement presenting the group’s position on the question. Some of the steps in the process are automated, while others are performed by the human participants. The HM plays the role of mediator, producing group statements which synthesize the perspectives of all the participants. This happens in two places:</p>
<ol>
<li><strong>Initial Phase</strong>: After seeing the question, participants are each asked to write their opinions. The HM takes these initial opinions and synthesizes them into a group statement. This is repeated to produce 4 different options, which the participants narrow down to a single statement via an election.<sup><a id="footnote-ref-er87dhu0426" href="#footnote-er87dhu0426" data-footnote-ref aria-describedby="footnote-label">3</a></sup></li>
<li><strong>Critique Phase</strong>: After seeing the winning group statement, participants are asked to give critiques. The HM takes these critiques and uses them to propose revised group statements, which the participants are again asked to vote on.</li>
</ol>
<p><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXeY5qvlo5IAX9zOYbc60M5IpGq2QrFsFeab-QTvJyLKwuV-NxMNUop0e6GVQ7067NYjEENjngg-YL0U8qbEJXrNolJObPklw2Jykwy9F0szBlvlUeAQUl3BEEPvVa_eOAW5B-G62A?key=aWp0E8rlx3pi0wuSPlVVoKs7" alt=""></p>
<p>This process ends with the participants selecting one group statement as their final choice. The participants are then also asked to reconsider their initial stance on the question, to evaluate whether they changed their minds or if a group consensus emerged.</p>
<h2>Automated Mediation</h2>
<p>The HM itself consists of two finetuned <a href="https://arxiv.org/abs/2203.15556">Chinchilla</a> models, which are used to generate a series of candidate group statements and narrow them down to a single group statement via a simulated election.</p>
<ul>
<li>Generative model (70B)</li>
<li>Reward model (1.4B)</li>
</ul>
<p>The machine produces a group statement by taking the following steps:</p>
<ol>
<li><strong>Generation</strong>: The generative model is prompted with the original question and the opinions provided by 5 human participants.<sup><a id="footnote-ref-5" href="#footnote-5" data-footnote-ref aria-describedby="footnote-label">4</a></sup> This is used to independently sample 16 candidate group statements.</li>
<li><strong>Prediction</strong>: The reward model is then used to predict how each person will rank the candidate group statements. The reward model is prompted with the question, a specific person’s opinion, and one of the candidate group statements. The model then outputs a score estimating how likely the person would rank that statement above the others.</li>
<li><strong>Simulated Election</strong>: The predicted rankings of candidate group statements are then used to run a simulated election, resulting in a single winning group statement.</li>
</ol>
<p><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXe3htQYKE3xjbE6S89OEnhXfC6CBxtW6bFhs_clysodmOBQ9cxRZqSbBWccXFcNSrQygS0OPnys2C-xalCL8kL2NGtA9u73VPQ2vFkjNRSzeog3Hzcv2O1CN0KUK3kEsikePoRy?key=aWp0E8rlx3pi0wuSPlVVoKs7" alt=""></p>
<p>The winner of the simulated election is the machine’s final output. </p>
<h1>Empirical Results</h1>
<p>The authors test their system by running live sessions with thousands of participants, all residents of the UK. The original research does a lot of different empirical evaluations of their system, which I omit for the sake of brevity. Here are some quick takeaways:</p>
<p>First, <strong>AI mediation does cause people to change their mind on the original question</strong>, and converge toward group consensus (people explicitly admit to their view changing). This does not happen without mediation (opinion exposure cohort). </p>
<p><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXewSl6eOQ5BZdYq9pZ-x8eDlE_wbcHL4aK-F7T-6AHSxZU8uIXUFeEoIVF1ZFuYVfLShVwlAaPQSJs7UBPcgIt4mrxffstmGch6UyxUe93n35wMaGqG4hCNDM8HPrm-1nTtPU1Vsg?key=aWp0E8rlx3pi0wuSPlVVoKs7" alt=""></p>
<p>Second, they show that the <strong>HM slightly outperforms human mediators</strong>, at least at writing statements that participants prefer. It performs about the same as human mediators at causing an increase in group agreement. It is important to note, however, that these were untrained mediators, sampled from the same pool of people selected as participants. </p>
<p><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXenqDoinJa4Pq6vyA409n6S_L1kxaLxp0rrjzcrYfmdKiCkteg8fiiWrb3UShoaEsUXuJw1lK87L1YMrMuUW71BE3OFBYMtksvysfWBYURFVEyJ3mbxQT9CVxQDeq1L-sqbISyTrA?key=aWp0E8rlx3pi0wuSPlVVoKs7" alt=""></p>
<h2>Comparison to Gemini 1.5 Pro</h2>
<p><em>Not included in the final paper (just the supplemental materials).</em></p>
<p>The authors also performed an evaluation of Gemini 1.5 Pro’s abilities, which seemed noteworthy. </p>
<p>The authors try to evaluate the effect of using a state of the art Gemini model, but by just prompting it and doing no finetuning. They compare how well both Gemini and the 1.4B reward model are able to predict human rankings, and they find that <strong>Gemini significantly underperforms the reward model.</strong> </p>
<p>Gemini is prompted with the question and a person’s initial opinion, and is then used to rank candidate group statements in two different ways:</p>
<ol>
<li>Comparing the average log probability of tokens in each candidate group statement (ranking them from highest to lowest average probability)<sup><a id="footnote-ref-4" href="#footnote-4" data-footnote-ref aria-describedby="footnote-label">5</a></sup>.</li>
<li>Giving all candidates to the model and asking explicitly for a ranking.</li>
</ol>
<p>This is done to predict individual rankings as well as aggregate rankings (by running a simulated election with individual rankings). In all cases Gemini does better than chance, but it always performs significantly worse than the finetuned reward model. I personally found this surprising, given the large difference in scale between the models.</p>
<h2>Embedding Space Analysis</h2>
<p>Finally, the authors also <strong>look for bias in group statements by examining the semantic embeddings</strong> of both the human opinions and generated outputs.</p>
<p>First, they identify a  “position component”  in the semantic embedding space which separates the affirming and negating answers to the original question (e.g. comparing the embeddings of “Voting should be compulsory” and “Voting should not be compulsory”).</p>
<p><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfSfZWWm4rMdlSbk_e8DjRytB7yLSq7WiqCDPH2LMzi6Ex_3s74pS-tXiPfAygDgmMlLJEYJgEhF7Zqm93a8LsLB5qJ4jO0kPgrBzys6G1lwD3BretSKuP0mvFD3qLhjsBt1zFHfg?key=aWp0E8rlx3pi0wuSPlVVoKs7" alt=""></p>
<p>They then show that group statements usually fall in between the opinion statements they were based on, suggesting that they tend to be a compromise between positions. </p>
<p><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXcycbQAdBzRu_1mO3uYGpbCuBqFsbBx-MKTlDId7U9Il_jB0_Nog9WXew5XgwbR1dwYFn_U93hHWVUhMy1rbqQ1aO5S8tIEUwpXfFGkcIERgTgvIFjmNY2geIWdIXjVVxF-KMs9Qg?key=aWp0E8rlx3pi0wuSPlVVoKs7" alt=""></p>
<p>They also examine the balance between the majority and minority clusters (partitioning the participants by their initial level of agreement with the question). They find no evidence, from comparing the embeddings, that the HM has any preference for the majority.</p>
<p>This kind of analysis is pretty ad hoc, but finding ways to demonstrate neutrality seems pretty important if a system like this were ever to be deployed in high stakes, or low trust contexts. Furthermore, as LLMs get better at mediating, there might be bigger concerns about adversarial persuasion and manipulation of participants toward one position or the other. </p>
<h1>Training Details</h1>
<p>Both the generative model and the reward model are finetuned from base models. The training data they used for finetuning is publicly available on <a href="https://github.com/google-deepmind/habermas_machine">Github</a>.</p>
<h2>Generative Model</h2>
<p>The generative model is finetuned in 3 iterations on <strong>human-curated samples of its own generations</strong>. Each iteration uses fresh human participants, but top statements are also included from previous iterations. Model-generated statements are filtered according to human provided quality scores (&gt;5 average on a 7 point scale), but it’s not clear from the paper what fraction of statements make the cut. </p>
<p><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXdWpZXoVaq7gQIj4MdPEK72nVbe0FDOXaXsgoMn30f4uQOrKcKVHVStx8PQSR8rKuuzLUcI6HDohbTY6cSpnsUr0IoiUkcivawkK0vPZIEGBLiYooSCBBkiMad2MrrnUie3kyXPtQ?key=aWp0E8rlx3pi0wuSPlVVoKs7" alt=""></p>
<p>Each iteration trains on roughly 5000 samples, running multiple epochs until a validation set shows signs of overfitting. The same model was trained to do both phases of the deliberation process (initial and critique phases).</p>
<h2>Reward Model</h2>
<p>For each set of human opinions, the generative model produces 4 candidate group statements which are then ranked by the participants, and the reward model is then trained to predict these rankings. The real human rankings are first converted into pairwise preferences, and the reward model is trained to fit a <a href="https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model">Bradley-Terry</a> preference model, estimating a score for each statement proportional to how likely it is to be preferred. </p>
<p>A linear layer is added on top of the 1.4B base model which outputs a scalar value. The model learns to compute this value from seeing a prompt including the question, the initial opinion response, and a single candidate group statement.</p>
<h1>Example Session</h1>
<p>The following is an example run of the deliberation process, taken from the supplementary materials. This session has been modified to compare 4 different ways to generate a group statement:</p>
<ul>
<li><strong>SFT+RM</strong>: This is the full HM, using both a model with supervised finetuning (SFT) and a reward model guided simulated election (RM). </li>
<li><strong>SFT</strong>: This uses supervised finetuning, but no simulated election.</li>
<li><strong>Base+RM</strong>: This uses a simulated election, but uses the original base model without finetuning. </li>
<li><strong>Base</strong>: This generates a group statement by just prompting the base model.</li>
</ul>
<p>Normally all 4 statements would be generated by the full HM, but this case lets us qualitatively compare what both finetuning and simulated elections are contributing to the statements.</p>
<h2>Question and Summary</h2>
<p>The group evaluates the question of compulsory voting, and converges toward the position that the government should not fine citizens who don&#39;t vote. Participant 1 still dissents with the majority opinion at the end.</p>
<p><img src="https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/7fbdc7351fce2c8b10863be1ef24d8acb27523af5361e6c1.png" alt=""></p>
<h2>Initial Phase</h2>
<p>Participants submit their opinions, and a group statement is chosen. In this case the HM (SFT + RM) produces the winning statement. The statement, while acknowledging the possible benefits of compulsory voting, seems to side primarily against it. Curiously, rather than just summarizing the human opinions, the <strong>HM offers a novel compromise position</strong> (not brought up by any of the participants), that the government should invest in non-coercive methods of increasing turnout instead. </p>
<p><img src="https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/5cf23132f2252916a69f3cc7ef6817334e6e47d1abfc7526.png" alt=""></p>
<h2>Critique Phase</h2>
<p>Participants submit their critiques of the winning group opinion, and a new revised group opinion is chosen. In this case the finetuned model which doesn&#39;t use a simulated election (SFT) produces the winning statement. The refined statements are all very similar to the winning statement from the previous phase, changing only minor details. Participant 1 has also stopped participating for some reason. </p>
<p><img src="https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/43aa39ce0eaefb9dfa7febe6b115e3f41450ad94d27d701d.png" alt=""></p>
<h2>Final Survey</h2>
<p>Four out of the five participants converged to the position that the government should NOT fine people for failing to vote. Participants also give some more detail about how their view changed.</p>
<p><img src="https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/807b0baebeaddb191e7627e17a9cc1960e5cd0bc23c34c16.png" alt=""></p>
<h1>Conclusion</h1>
<p>Hopefully this quick overview at least gives a rough sense of what they’ve done, and maybe some clues about where a method like this might go in the future. While this work is focused primarily on helping people to find common ground on a variety of UK-relevant social issues, this research could develop in a lot of different directions. Future versions of this kind of AI-mediated dialogue could potentially have a place in improving negotiations, resolving intellectual disagreements between researchers, or accelerating public consensus around AI policy questions.   </p>
<p>This work uses an LLM which is fairly rudimentary (by 2025 standards), and so it seems likely that future models with improved reasoning abilities could be used to play a much bigger role in improving collective dialogue, depending in large part on the order in which the relevant capabilities emerge.</p>
<section class="footnotes" data-footnotes>
<h2 id="footnote-label" class="sr-only">Footnotes</h2>
<ol>
<li id="footnote-vgzvc1nrpo">
<p>The Habermas Machine is named after <a href="https://en.wikipedia.org/wiki/The_Theory_of_Communicative_Action">Jürgen Habermas</a>.  <a href="#footnote-ref-vgzvc1nrpo" data-footnote-backref aria-label="Back to reference vgzvc1nrpo">↩</a></p>
</li>
<li id="footnote-txqquee37d">
<p>Participants are asked if they agree with either the affirming or negating rephrasing of the question (“Voting should be compulsory” vs “Voting should not be compulsory”). This question is asked both before and after deliberating, to assess whether they changed their minds during the process, and if the group converged toward a shared opinion.  <a href="#footnote-ref-txqquee37d" data-footnote-backref aria-label="Back to reference txqquee37d">↩</a></p>
</li>
<li id="footnote-er87dhu0426">
<p>All elections in the paper, simulated or real, use the <a href="https://en.wikipedia.org/wiki/Schulze_method">Schulze voting rule</a>. This method was chosen because of 1) independence of “clones” and 2) robustness to strategic voting. <a href="#footnote-ref-er87dhu0426" data-footnote-backref aria-label="Back to reference er87dhu0426">↩</a></p>
</li>
<li id="footnote-5">
<p>The number of participants is usually 5, but the system is also trained to be robust to smaller groups. <a href="#footnote-ref-5" data-footnote-backref aria-label="Back to reference 5">↩</a></p>
</li>
<li id="footnote-4">
<p>This is meant to be a proxy for evaluating Gemini as a generator model. This is a pretty strange method, and I&#39;m personally not surprised that it failed to work. <a href="#footnote-ref-4" data-footnote-backref aria-label="Back to reference 4">↩</a></p>
</li>
</ol>
</section>

    </article>
    <hr>
    <newsletter-component></newsletter-component>
    <hr>
    <footer-component></footer-component>
  </div>
  <script>
    document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body, {
        delimiters: [
          {left: "$", right: "$", display: false},
          {left: "$$", right: "$$", display: true}
        ]
      });
    });
  </script>
</body>
</html>
